{
    "_comments": [
        "Reminders and tips:",
        " * For your data, use nx=1, and use a long ny like 32768.",
        " * For this model, it really helps if you have the delay in your data set",
        "   correctly. I've seen improvements fixing a delay that was off by 10",
        "   samples.",
        " * gamma below is picked so that we end up with a learning rate of about",
        "   1e-4 after 1000 epochs. I've found LSTMs to work with a pretty aggressive",
        "   learning rate that would be out of the question for other architectures.",
        " * Number of units between 8 and 96, layers from 1 to 5 all seem to be ok",
        "   depending on the dataset, though bigger models might not make real-time."
    ],
    "net": {
        "name": "LSTM",
        "config": {
            "hidden_size": 24,
            "train_burn_in": 4096,
            "train_truncate": 1024,
            "num_layers": 3
        }
    },
    "optimizer": {
        "lr": 0.01
    },
    "lr_scheduler": {
        "class": "ExponentialLR",
        "kwargs": {
            "gamma": 0.995
        }
    }
}